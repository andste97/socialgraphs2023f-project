{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data analysis metrics\n",
    "\n",
    "> An explanation of the central idea behind your final project (What is the idea? Why is it interesting? Which datasets did you need to explore the idea? How did you download them?)\n",
    "\n",
    "Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up outputs from warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scraper\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist\n",
    "import pathlib\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something related to tqdm fails, run:\n",
    "> `pip install ipywidgets widgetsnbextension pandas-profiling`\n",
    "\n",
    "Changes?\n",
    "> `pip freeze > requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 categories: 100%|██████████| 3/3 [00:00<00:00,  3.81it/s]\n",
      "Fetching 1001 page archive titles: 100%|██████████| 1001/1001 [00:03<00:00, 302.38it/s]\n",
      "Fetching 7358 talk pages: 100%|██████████| 148/148 [00:11<00:00, 12.90it/s]\n",
      "Writing talk page batches to disk: 100%|██████████| 148/148 [00:02<00:00, 58.31it/s]\n",
      "Parsing talk page batches: 100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\n",
      "Fetching 1001 wiki pages: 100%|██████████| 21/21 [00:03<00:00,  6.18it/s]\n",
      "Parsing wiki page batches: 100%|██████████| 21/21 [00:05<00:00,  4.00it/s]\n",
      "Creating graph: 100%|██████████| 7358/7358 [00:00<00:00, 10015.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges: 210653\n"
     ]
    }
   ],
   "source": [
    "category_titles = [\n",
    "    \"Category:Wikipedia_level-1_vital_articles\",\n",
    "    \"Category:Wikipedia_level-2_vital_articles\",\n",
    "    \"Category:Wikipedia_level-3_vital_articles\",\n",
    "    #\"Category:Wikipedia_level-4_vital_articles\",\n",
    "    #\"Category:Wikipedia_level-5_vital_articles\"\n",
    "]\n",
    "\n",
    "page_graph, infos  = await scraper.scrape_wiki(category_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A walk-through of your preliminary data analysis, addressing:\n",
    "> - What is the total size of your data? (MB, number of pages, other variables, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['titles', 'archive_titles'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the vital articles dataset: 1001\n",
      "Number of related archived pages: 6357\n",
      "Number of users found in relation to the dataset: 79523\n",
      "No. nodes 81525\n",
      "No. links: 162129\n"
     ]
    }
   ],
   "source": [
    "talk_pages = [title for title in page_graph.nodes if title[:5] == \"Talk:\"]\n",
    "users = [title for title in page_graph.nodes if title[:5] == \"User:\"]\n",
    "print(\"Number of pages in the vital articles dataset:\", len(infos[\"titles\"]))\n",
    "print(\"Number of related archived pages:\", len(infos[\"archive_titles\"]))\n",
    "print(\"Number of users found in relation to the dataset:\", len(users))\n",
    "print(\"No. nodes\", page_graph.number_of_nodes())\n",
    "print(\"No. links:\", page_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - What is the network you will be analyzing? (number of nodes? number of links?, degree distributions, what are node attributes?, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = 'DejaVu Sans'\n",
    "\n",
    "graph = page_graph.copy()\n",
    "\n",
    "# Only keep users with a high degree\n",
    "for node in page_graph.nodes(data=True):\n",
    "    if \"page_class\" in node[1]:\n",
    "        if node[1][\"page_class\"] == \"user\":\n",
    "            if page_graph.out_degree(node[0]) <= 10:\n",
    "                graph.remove_node(node[0])\n",
    "    else:\n",
    "        graph.remove_node(node[0])\n",
    "\n",
    "# Remove outliers\n",
    "cc = nx.weakly_connected_components(graph)\n",
    "largest_c = max(cc, key=lambda x: len(x))\n",
    "rsubgraph = nx.subgraph(graph, largest_c)\n",
    "\n",
    "# Positions (currently unused)\n",
    "#pos = nx.nx_agraph.graphviz_layout(rsubgraph, prog=\"neato\")\n",
    "\n",
    "# Color and size according to coast and degree\n",
    "color_talk = \"#0000FF\"\n",
    "color_user = \"#FF0000\"\n",
    "node_colors = [color_talk if node[1][\"page_class\"] == \"talk\" else color_user for node in rsubgraph.nodes(data=True)]\n",
    "node_sizes = [rsubgraph.degree(node) for node in rsubgraph.nodes]\n",
    "\n",
    "nx.draw(rsubgraph, with_labels=True, font_weight='light', font_size=5, node_size=node_sizes, width=.1, edge_color=\"#555555\", arrowsize=2, node_color=node_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(rsubgraph.degree, key=lambda item: item[1], reverse=True)[:10]\n",
    "import pandas as pd\n",
    "\n",
    "degrees = pd.DataFrame(rsubgraph.degree, columns=[\"Node\", \"Degree\"])\n",
    "degrees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees[\"PageType\"] = [\"User\" if node[:5] == \"User:\" else \"Talk\" for node in degrees.Node ]\n",
    "users = degrees[degrees.PageType == \"User\"]\n",
    "potential_bots = users[users[\"Node\"].str.contains('bot', case=False)]\n",
    "print(f\"{len(potential_bots)} users found with with bot in their name:\")\n",
    "print(\",\\n\".join(potential_bots.Node.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluation we found that `User:Botteville`, `User:KP Botany` and `User:NinjaRobotPirate` are human users. Therefore we can filter out the bots by name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_names = [\"User:Community Tech bot\", \"User:PrimeBOT\", \"User:InternetArchiveBot\", \"User:AnomieBOT\", \"User:RMCD bot\", \"User:Cyberbot II\", \"User:CommonsNotificationBot\",\n",
    "\"User:LinkBot\", \"User:FairuseBot\", \"User:BetacommandBot\", \"User:Legobot\", \"User:DumZiBoT\"]\n",
    "\n",
    "human_users = users[~users[\"Node\"].isin(bot_names)]\n",
    "n = 10\n",
    "top_n_human_df = human_users.sort_values([\"Degree\"], ascending=False).head(n)\n",
    "top_n_human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make subgraph with users and their related pages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_human_names = top_n_human_df.Node.values\n",
    "top_users_graph = graph.subgraph(sum([list(graph.neighbors(node)) + [node] for node in top_n_human_names], []))\n",
    "pages = [node for node in top_users_graph.nodes if node not in top_n_human_names] \n",
    "\n",
    "node_colors = [color_talk if node[1][\"page_class\"] == \"talk\" else color_user for node in top_users_graph.nodes(data=True)]\n",
    "node_sizes = [rsubgraph.degree(node) for node in top_users_graph.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(top_users_graph.degree())\n",
    "sorted_nodes = sorted(degrees, key=degrees.get, reverse=True)\n",
    "top_nodes_count = 10\n",
    "top_nodes = [node for node in sorted(degrees, key=degrees.get, reverse=True) if node[:5] == \"Talk:\"][:top_nodes_count]\n",
    "labels = {node: node for node in top_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fa2 import ForceAtlas2\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=2.0,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=3.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=0.1,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "pos=forceatlas2.forceatlas2_networkx_layout(top_users_graph, pos=None, iterations=2000)\n",
    "    \n",
    "nx.draw(top_users_graph, pos=pos, node_color=node_colors, node_size=node_sizes, edge_color=\"#999999\", width=0.3, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(top_users_graph)\n",
    "label_pos = {node: (pos[node][0], pos[node][1] + 0.15) for node in top_nodes}\n",
    "nx.draw(top_users_graph, pos=pos, node_color=node_colors, node_size=node_sizes, edge_color=\"#999999\", width=0.3, alpha=0.5, with_labels=False)\n",
    "nx.draw_networkx_labels(top_users_graph, pos, labels=labels, font_size=10, font_color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "import scipy.stats as sps\n",
    "\n",
    "# Basic Statistics\n",
    "\n",
    "count_nodes = len(page_graph)\n",
    "count_edges = len(page_graph.edges())\n",
    "\n",
    "# Create degree statistic dicts\n",
    "degrees = dict(page_graph.degree())\n",
    "in_degrees = dict(page_graph.in_degree())\n",
    "out_degrees = dict(page_graph.out_degree())\n",
    "\n",
    "talk_page_in_degrees = {k: v for k, v in in_degrees.items() if page_graph.nodes[k][\"page_class\"] == \"talk\"}\n",
    "user_out_degrees = {k: v for k, v in out_degrees.items() if page_graph.nodes[k][\"page_class\"] == \"user\"}\n",
    "\n",
    "def find_top(n, stat_dict):\n",
    "    degrees_pages = []\n",
    "    degrees_users = []\n",
    "    top_overall = \"\"\n",
    "\n",
    "    for page, degree in dict(sorted(stat_dict.items(), key=lambda item: item[1], reverse=True)).items():\n",
    "        if top_overall == \"\":\n",
    "            top_overall = page + \" - \" + str(degree)\n",
    "\n",
    "        # stat dicts don't distinguish between east/west, so we'll do that here\n",
    "        if len(degrees_pages) < n and page_graph.nodes[page][\"page_class\"] == \"talk\": \n",
    "            degrees_pages.append(page + \" - \" + str(degree))\n",
    "        elif len(degrees_users) < n and page_graph.nodes[page][\"page_class\"] == \"user\":\n",
    "            degrees_users.append(page + \" - \" + str(degree))\n",
    "\n",
    "        if len(degrees_pages) >= n and len(degrees_users) >= n:\n",
    "            break  # found all top v\n",
    "    \n",
    "    return degrees_pages, degrees_users, top_overall\n",
    "\n",
    "degrees_pages, degrees_users, top_overall = find_top(10, degrees)\n",
    "\n",
    "print(\"Number of nodes: \" + str(count_nodes))\n",
    "print(\"Number of links: \" + str(count_edges))\n",
    "\n",
    "print()\n",
    "print(\"Highest degrees for pages:\")\n",
    "print(\"> Overall:\")\n",
    "print(top_overall)\n",
    "print(\"> Pages:\")\n",
    "print(\"\\n\".join(degrees_pages))\n",
    "print(\"> Users:\")\n",
    "print(\"\\n\".join(degrees_users))\n",
    "\n",
    "\n",
    "# Degree multiplicities\n",
    "in_degrees_counts = p.Series(talk_page_in_degrees.values()).value_counts()\n",
    "out_degrees_counts = p.Series(user_out_degrees.values()).value_counts()\n",
    "\n",
    "max_degree = max([max(in_degrees_counts.index), max(out_degrees_counts.index)])\n",
    "max_multiplicity = max([max(in_degrees_counts.values), max(out_degrees_counts.values)])\n",
    "range_x = range(1, max_degree + 1)\n",
    "\n",
    "in_degrees_counts_interp = in_degrees_counts.reindex(range(max_degree+1), fill_value=0).sort_index()\n",
    "out_degrees_counts_interp = out_degrees_counts.reindex(range(max_degree+1), fill_value=0).sort_index()\n",
    "\n",
    "# Exponents\n",
    "fit_in = powerlaw.Fit(in_degrees_counts.sort_index().values, verbose=False)\n",
    "fit_out = powerlaw.Fit(out_degrees_counts.sort_index().values, verbose=False)\n",
    "\n",
    "exp_in = fit_in.alpha\n",
    "exp_out = fit_out.alpha\n",
    "\n",
    "print(\"Exponents:\")\n",
    "print(\"In-degrees: \" + str(exp_in) + \" sigma: \" + str(fit_in.sigma))\n",
    "print(\"Out-degrees: \" + str(exp_out) + \" sigma: \" + str(fit_out.sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def fpl(x, a):\n",
    "    return x ** (-a)\n",
    "\n",
    "# Plots\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "# Sturges rule\n",
    "no_bins_sturges = int(1 + math.log(len(talk_page_in_degrees.values()), 2))\n",
    "\n",
    "axs[0, 0].scatter(in_degrees_counts.index, in_degrees_counts.values, s=5, label='Data')\n",
    "#hist, bin_edges, _ = axs[0, 0].hist(talk_page_in_degrees.values(), bins=no_bins_sturges, edgecolor='white', label='Data')\n",
    "#axs[0, 0].set_xticks(bin_edges)\n",
    "#axs[0, 0].set_xticklabels(['%.0f' % val for val in bin_edges], rotation=45)\n",
    "axs[0,0].set_title('Multiplicity of In-degrees for Talk pages')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].scatter(in_degrees_counts.index, in_degrees_counts.values, s=5, label='Data')\n",
    "#axs[0,1].plot(range_x, fpl(range_x, exp_in) * count_nodes, 'k-', lw=1, alpha=.75, label='Power Law fit')\n",
    "axs[0,1].set_yscale('log')\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,1].set_title('Multiplicity of In-degrees for Talk pages [log-log]')\n",
    "axs[0,1].set_xlim(1, max_degree)\n",
    "axs[0,1].set_ylim(1, max_multiplicity)\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1, 0].scatter(out_degrees_counts.index, out_degrees_counts.values, color=\"red\", s=5, label='Data')\n",
    "# hist, bin_edges, _ = axs[1, 0].hist(talk_page_in_degrees.values(), bins=no_bins_sturges, color=\"red\", edgecolor='white', label='Data')\n",
    "# axs[1, 0].set_xticks(bin_edges)\n",
    "# axs[1, 0].set_xticklabels(['%.0f' % val for val in bin_edges], rotation=45)\n",
    "axs[1,0].set_title('Multiplicity of Out-degrees for Users')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].scatter(out_degrees_counts.index, out_degrees_counts.values, s=5, label='Data', color=\"red\")\n",
    "#axs[1,1].plot(range_x, fpl(range_x, exp_out) * count_nodes, 'k-', lw=1, alpha=.75, label='Power Law fit')\n",
    "axs[1,1].set_yscale('log')\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,1].set_title('Multiplicity of Out-degrees for Users [log-log]')\n",
    "axs[1,1].set_xlim(1, max_degree)\n",
    "axs[1,1].set_ylim(1, max_multiplicity)\n",
    "axs[1,1].legend()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Degree', ylabel='Multiplicity')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis per user\n",
    "\n",
    "For this, we will first need to extract all the comments from all pages, as well as the author of the comment\n",
    "Then we will assign all comment texts to a single author, and run sentiment analysis on the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to parse: page_contents/Talk:Economy.txt\n",
      "failed to parse: page_contents/Talk:City.txt\n",
      "failed to parse: page_contents/Talk:Sport of athletics.txt\n",
      "failed to parse: page_contents/Talk:Scientific Revolution.txt\n",
      "failed to parse: page_contents/Talk:Weak interaction.txt\n",
      "failed to parse: page_contents/Talk:Crustacean.txt\n",
      "failed to parse: page_contents/Talk:The Buddha.txt\n",
      "failed to parse: page_contents/Talk:Natural rubber.txt\n",
      "failed to parse: page_contents/Talk:News.txt\n",
      "failed to parse: page_contents/Talk:Japan/Archive 9.txt\n",
      "failed to parse: page_contents/Talk:Immanuel Kant/Archive 6.txt\n",
      "failed to parse: page_contents/Talk:Johann Sebastian Bach/Archive 15.txt\n",
      "failed to parse: page_contents/Talk:Argentina/Archive 7.txt\n",
      "failed to parse: page_contents/Talk:United Kingdom/Archive 25.txt\n",
      "failed to parse: page_contents/Talk:Mahatma Gandhi/Archive 9.txt\n",
      "failed to parse: page_contents/Talk:Wolfgang Amadeus Mozart/Archive 2.txt\n",
      "failed to parse: page_contents/Talk:Mahatma Gandhi/Archive 8.txt\n",
      "failed to parse: page_contents/Talk:Mahatma Gandhi/Archive 17.txt\n",
      "failed to parse: page_contents/Talk:Stroke/Archive 2.txt\n",
      "failed to parse: page_contents/Talk:Mahatma Gandhi/Archive 6.txt\n",
      "failed to parse: page_contents/Talk:China/Archive 16.txt\n",
      "failed to parse: page_contents/Talk:Tool/Archive 1.txt\n",
      "failed to parse: page_contents/Talk:Sexual intercourse/Archive 3.txt\n",
      "failed to parse: page_contents/Talk:Spanish Empire/Archive 5.txt\n",
      "failed to parse: page_contents/Talk:China/Archive 4.txt\n",
      "failed to parse: page_contents/Talk:History of India/Archive 4.txt\n",
      "failed to parse: page_contents/Talk:Napoleon/Archive 6.txt\n",
      "failed to parse: page_contents/Talk:Napoleon/Archive 5.txt\n",
      "failed to parse: page_contents/Talk:Truth/Archive 15.txt\n",
      "failed to parse: page_contents/Talk:Hong Kong/Archive 5.txt\n",
      "failed to parse: page_contents/Talk:Line (geometry)/Archive 2.txt\n",
      "failed to parse: page_contents/Talk:Racism/Archive 25.txt\n",
      "failed to parse: page_contents/Talk:Suleiman the Magnificent/Archive 1.txt\n"
     ]
    }
   ],
   "source": [
    "wikipage_folder = pathlib.Path(\"./page_contents/\")\n",
    "filenames = list(wikipage_folder.rglob(\"*.txt\"))\n",
    "\n",
    "#worker_results = [parse_comments_from_pages(filenames[:100])]\n",
    "\n",
    "with Pool(12) as pool:\n",
    "    # perform calculations\n",
    "    worker_results = pool.map(utils.parse_comments_from_pages, utils.chunk_list(filenames[:2000], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_or_create_list_in_dict(dict, key, value):\n",
    "    if(key not in dict):\n",
    "        dict[key] = []\n",
    "    \n",
    "    dict[key].append(value)\n",
    "\n",
    "def append_comment_to_talkpage(page_dict, filepath, comment):\n",
    "    '''Append a comment to a list of comments on that talk page. Will collect all comments for a specific talk\n",
    "    page, including comments archived talk pages'''\n",
    "    filepath_parts = filepath.split('/')\n",
    "\n",
    "    if(len(filepath_parts) > 2):\n",
    "        append_or_create_list_in_dict(page_dict, filepath_parts[1], comment)\n",
    "\n",
    "    else:\n",
    "        filename_parts = filepath_parts[1].split('.')\n",
    "        append_or_create_list_in_dict(page_dict,filename_parts[0], comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict = {}\n",
    "list_for_df = []\n",
    "page_dict = {}\n",
    "\n",
    "# iterate over the results by the workers\n",
    "# and transform the output into a dictionary with the users as keys\n",
    "# and their comments as text\n",
    "for worker_result in worker_results:\n",
    "    for filepath, page in worker_result:\n",
    "        for subsection in page[\"sections\"]:\n",
    "            if subsection.get(\"heading\"):\n",
    "                for comments in subsection.get(\"comments\"):\n",
    "                    if comments.get(\"author\"):\n",
    "                        for author, comment in utils.parse_comment_subcomment(comments):\n",
    "                            append_or_create_list_in_dict(author_dict, author, comment)\n",
    "                            append_comment_to_talkpage(page_dict, filepath, comment)\n",
    "                            list_for_df.append([author, comment, filepath])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze sentiment on per-author basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rick Norwood', 449219),\n",
       " ('RJII', 385753),\n",
       " ('Peter jackson', 528358),\n",
       " ('RJHall', 309918),\n",
       " ('DeCausa', 287810)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "# show the top 5 authors written the most text in comment pages\n",
    "# before tokenizing the comments\n",
    "items = author_dict.items()\n",
    "items_sorted = sorted(items, key=lambda x: len(x[1]), reverse=True)\n",
    "[(author, len(utils.flatten(comments))) for author, comments in items_sorted][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT = pd.read_csv(\"./labMT.txt\", sep=\"\\t\")\n",
    "# to facilitate happiness_average value lookup\n",
    "labMT.set_index(\"word\", inplace=True)\n",
    "\n",
    "# Do sentiment analysis\n",
    "# code taken from assignment 2\n",
    "\n",
    "def sentiment(tokens):\n",
    "    if(len(tokens) == 0):\n",
    "        return\n",
    "    freq = FreqDist(tokens)\n",
    "\n",
    "    # filter for the vocabulary we can evaluate with LabMT\n",
    "    vocab = list(filter(lambda word: word in labMT.index, np.unique(tokens)))\n",
    "\n",
    "    # array of each token's average happiness weighted by the token's frequency\n",
    "    weighted_happiness = np.fromiter((freq[word] * labMT.loc[word].happiness_average for word in vocab), dtype=float)\n",
    "    # each token's frequency\n",
    "    word_frequencies = np.fromiter((freq[word] for word in vocab), dtype=float)\n",
    "    return np.sum(weighted_happiness) / np.sum(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_happiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Author</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dolphin51</th>\n",
       "      <td>5.364909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rracecarr</th>\n",
       "      <td>5.376242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spinningspark</th>\n",
       "      <td>5.381130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MrAureliusR</th>\n",
       "      <td>5.277685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVdm</th>\n",
       "      <td>5.445121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               comment_happiness\n",
       "Author                          \n",
       "Dolphin51               5.364909\n",
       "Rracecarr               5.376242\n",
       "Spinningspark           5.381130\n",
       "MrAureliusR             5.277685\n",
       "DVdm                    5.445121"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = {}\n",
    "for author, text in author_dict.items():\n",
    "    text = [utils.tokenize_custom(s) for s in text]\n",
    "    text = utils.flatten(text)\n",
    "    # compute sentiment for individual rapper wiki page\n",
    "    if len(text) > 100:\n",
    "        sentiment_value = sentiment(text)\n",
    "        if(sentiment_value):\n",
    "            sentiments[author] = sentiment_value\n",
    "\n",
    "sentiments_df = pd.DataFrame({\"Author\": sentiments.keys(), \"comment_happiness\": sentiments.values()})\n",
    "# use the author's name as index\n",
    "sentiments_df.set_index(\"Author\", inplace=True)\n",
    "sentiments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 authors with the happiest comments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_happiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Author</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lia 199712</th>\n",
       "      <td>5.897957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2A02:CE0:1801:50E:C51D:9330:A910:98D3</th>\n",
       "      <td>5.879540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bkusmono</th>\n",
       "      <td>5.855294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76.2.40.115</th>\n",
       "      <td>5.841782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balboadancer</th>\n",
       "      <td>5.769604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.216.66.6</th>\n",
       "      <td>5.765306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Godar75</th>\n",
       "      <td>5.758889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lwarnecke</th>\n",
       "      <td>5.758868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Praxaquilani</th>\n",
       "      <td>5.757143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24.115.80.11</th>\n",
       "      <td>5.748718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       comment_happiness\n",
       "Author                                                  \n",
       "Lia 199712                                      5.897957\n",
       "2A02:CE0:1801:50E:C51D:9330:A910:98D3           5.879540\n",
       "Bkusmono                                        5.855294\n",
       "76.2.40.115                                     5.841782\n",
       "Balboadancer                                    5.769604\n",
       "80.216.66.6                                     5.765306\n",
       "Godar75                                         5.758889\n",
       "Lwarnecke                                       5.758868\n",
       "Praxaquilani                                    5.757143\n",
       "24.115.80.11                                    5.748718"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 authors with the happiest comments:\")\n",
    "best_sentiments = sentiments_df.sort_values(\"comment_happiness\", ascending=False).head(10)\n",
    "best_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 authors with the saddest comments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_happiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Author</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93.229.148.172</th>\n",
       "      <td>4.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yuvalkatz</th>\n",
       "      <td>4.878079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71.211.175.77</th>\n",
       "      <td>4.886935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bertus</th>\n",
       "      <td>4.903033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoxbar</th>\n",
       "      <td>4.904769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dallas84</th>\n",
       "      <td>4.921635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guard Chasseur</th>\n",
       "      <td>4.930728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108.170.154.240</th>\n",
       "      <td>4.951125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongShaunSilvr</th>\n",
       "      <td>4.981053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200.85.115.19</th>\n",
       "      <td>4.987377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 comment_happiness\n",
       "Author                            \n",
       "93.229.148.172            4.734500\n",
       "Yuvalkatz                 4.878079\n",
       "71.211.175.77             4.886935\n",
       "Bertus                    4.903033\n",
       "Hoxbar                    4.904769\n",
       "Dallas84                  4.921635\n",
       "Guard Chasseur            4.930728\n",
       "108.170.154.240           4.951125\n",
       "LongShaunSilvr            4.981053\n",
       "200.85.115.19             4.987377"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 authors with the saddest comments:\")\n",
    "worst_sentiments = sentiments_df.sort_values(\"comment_happiness\", ascending=True).head(10)\n",
    "worst_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' == Semi-protected edit request on 29 May 2020 ==\\n \\n {{edit semi-protected|Great Barrier Reef|answered=yes}}\\n One of Australia’s most remarkable natural gifts, the Great Barrier Reef is blessed with the breathtaking beauty of the world’s largest coral reef. The reef contains an abundance of marine life and comprises of over 3000 individual reef systems and coral cays and literally hundreds of picturesque tropical islands with some of the worlds most beautiful sun-soaked, golden beaches.\\n Because of its natural beauty, the Great Barrier Reef has become one of the worlds most sought after tourist destinations.A visitor to the Great Barrier Reef can enjoy many experiences including snorkelling, scuba diving, aircraft or helicopter tours, bare boats (self-sail), glass-bottomed boat viewing, semi-submersibles and educational trips, cruise ship tours, whale watching and swimming with dolphins [[User:Lia 199712|Lia 199712]] ([[User talk:Lia 199712|talk]]) 02:25, 29 May 2020 (UTC)\\n']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this shows the original comment before tokenizin\n",
    "[s for s in author_dict.get(best_sentiments.iloc[0].name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' == Copper Vectoring ==\\n \\n Im Artikel steht der Satz \\'\\'\\'\"Kupfer leitet den elektrischen Strom sehr gut (58 · 106 S/m).\"\\'\\'\\' \\n <br>Das Kupfernetz der Deutschen Telekom in Deutschland besteht zu 100 Prozent aus Kupfer. Die elektrische Spannung V beträgt zwischen 60-70 Volt. Der elektrische Stromfluss A ist variabel (Elektronik bis 5 Ampere). Die Bundesnetzagentur hat entschieden, dass im Netz der Deutschen Telekom der \\'\\'\\'Bitstrom\\'\\'\\' (Datenpakete) mit 100 MBit/s (300 MBit/s - Leitung/virtuelle Leitung/Leitung-) fließen soll. Dazu investiert die Deutsche Telekom ab 2013 zirka 200 Euro pro Haushalt, bei 24 Millionen Haushalten sind das  4,8 Milliarden Euro. Dabei wird die Kabelverteilertechnik in 330.000 Kabelverteilern in Deutschland durch Technik der Firma [[Alcatel Lucent]] ersetzt. Auf Seiten der Telekomkunden und Mitbewerberkunden werden entsprechende 100 MBit/s-Modems benötigt.  [[Special:Contributions/93.229.148.172|93.229.148.172]] ([[User talk:93.229.148.172|talk]]) 01:28, 5 September 2013 (UTC)\\n']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in author_dict.get(worst_sentiments.iloc[0].name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from per-author sentiment analysis\n",
    "\n",
    "What we have seen is that comment sentiment mostly correlates with the topic they are discussing. E.g.: topics discussing the pages fear, Nazi, Hitler have a very low score, but comments discussing e.g. happiness, or the great barrier reef have very high sentiments.\n",
    "\n",
    "These results show, that it is not beneficial to use the labMT approach to find toxic comments. Different methods should be used for our problem.\n",
    "To show this also concretely, we will show a statistical correlation between talk page sentiment & article page sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-talkpage sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_happiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Talk:Force</th>\n",
       "      <td>5.344659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Mediterranean Sea</th>\n",
       "      <td>5.391800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Orbit</th>\n",
       "      <td>5.323326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Dante Alighieri</th>\n",
       "      <td>5.413513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Pablo Picasso</th>\n",
       "      <td>5.554139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        comment_happiness\n",
       "page                                     \n",
       "Talk:Force                       5.344659\n",
       "Talk:Mediterranean Sea           5.391800\n",
       "Talk:Orbit                       5.323326\n",
       "Talk:Dante Alighieri             5.413513\n",
       "Talk:Pablo Picasso               5.554139"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments_pages = {}\n",
    "for pagename, text in page_dict.items():\n",
    "    text = [utils.tokenize_custom(s) for s in text]\n",
    "    text = utils.flatten(text)\n",
    "    # compute sentiment for individual rapper wiki page\n",
    "    if len(text) > 100:\n",
    "        sentiment_value = sentiment(text)\n",
    "        if(sentiment_value):\n",
    "            sentiments_pages[pagename] = sentiment_value\n",
    "\n",
    "sentiments_pages_df = pd.DataFrame({\"page\": sentiments_pages.keys(), \"comment_happiness\": sentiments_pages.values()})\n",
    "# use the author's name as index\n",
    "sentiments_pages_df.set_index(\"page\", inplace=True)\n",
    "sentiments_pages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 happiest talk pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_happiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Talk:Book</th>\n",
       "      <td>5.656556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Play (activity)</th>\n",
       "      <td>5.640506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Solar energy</th>\n",
       "      <td>5.629508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Computer science</th>\n",
       "      <td>5.616852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Ecology</th>\n",
       "      <td>5.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Dream</th>\n",
       "      <td>5.606200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Memory</th>\n",
       "      <td>5.577665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Colombia</th>\n",
       "      <td>5.564483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Intelligence</th>\n",
       "      <td>5.562511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Television</th>\n",
       "      <td>5.558110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       comment_happiness\n",
       "page                                    \n",
       "Talk:Book                       5.656556\n",
       "Talk:Play (activity)            5.640506\n",
       "Talk:Solar energy               5.629508\n",
       "Talk:Computer science           5.616852\n",
       "Talk:Ecology                    5.611111\n",
       "Talk:Dream                      5.606200\n",
       "Talk:Memory                     5.577665\n",
       "Talk:Colombia                   5.564483\n",
       "Talk:Intelligence               5.562511\n",
       "Talk:Television                 5.558110"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 happiest talk pages:\")\n",
    "best_sentiments = sentiments_pages_df.sort_values(\"comment_happiness\", ascending=False).head(10)\n",
    "best_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 saddest talk pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_happiness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Talk:Common cold</th>\n",
       "      <td>5.118947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Mental disorder</th>\n",
       "      <td>5.122759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Hospital</th>\n",
       "      <td>5.158393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Terrorism</th>\n",
       "      <td>5.168383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Nervous system</th>\n",
       "      <td>5.170948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Cold War</th>\n",
       "      <td>5.171320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Allergy</th>\n",
       "      <td>5.173241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:War</th>\n",
       "      <td>5.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Fear</th>\n",
       "      <td>5.188344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talk:Disease</th>\n",
       "      <td>5.220673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      comment_happiness\n",
       "page                                   \n",
       "Talk:Common cold               5.118947\n",
       "Talk:Mental disorder           5.122759\n",
       "Talk:Hospital                  5.158393\n",
       "Talk:Terrorism                 5.168383\n",
       "Talk:Nervous system            5.170948\n",
       "Talk:Cold War                  5.171320\n",
       "Talk:Allergy                   5.173241\n",
       "Talk:War                       5.185000\n",
       "Talk:Fear                      5.188344\n",
       "Talk:Disease                   5.220673"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 saddest talk pages:\")\n",
    "worst_sentiments = sentiments_pages_df.sort_values(\"comment_happiness\", ascending=True).head(10)\n",
    "worst_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract article page sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-page sentiment analysis results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic comment extraction & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.DataFrame(list_for_df, columns = ['Author', 'Comment', 'Filename']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for loading pickles\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dictionary values\n",
    "#flattened_data = [(key, value) for key, values in author_dict.items() for value in values]\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "#df = pd.DataFrame(flattened_data, columns=['Author', 'Comment'])\n",
    "\n",
    "# load the vectorizer\n",
    "with open('./sentiment-models/vectorizer.pkl', 'rb') as file:\n",
    "    vec = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse TF-IDF matrix with vectorizer trained on kaggle toxic comment dataset\n",
    "\n",
    "comments_sparse = vec.transform(df_comments['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict toxic\n",
      "predict severe_toxic\n",
      "predict obscene\n",
      "predict threat\n",
      "predict insult\n",
      "predict identity_hate\n"
     ]
    }
   ],
   "source": [
    "# Classify comments according to the following categories:\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "preds = np.zeros((len(df_comments), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('predict', j)\n",
    "    with open(f'./sentiment-models/{j}.pkl', 'rb') as file:\n",
    "        m,r = pickle.load(file)\n",
    "    preds[:,i] = m.predict_proba(comments_sparse.multiply(r))[:,1]\n",
    "    df_comments[j] = preds[:,i]\n",
    "\n",
    "# this created a matrix where each new column for label_cols contains the probability that a comment is in\n",
    "# the category with the same title as the column name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_contents/Talk:Hormone.txt 5.103.42.242 0.9200454171210526  == Are you dumb ==\n",
      " \n",
      " Idk [[Special:Contributions/5.103.42.242|5.103.42.242]] ([[User talk:5.103.42.242|talk]]) 18:29, 16 April 2023 (UTC)\n",
      "page_contents/Talk:Scramble for Africa.txt DePiep 0.9962537910110114  :::::Sure you don't have to reply. But unless you ''quote me saying what you put in my mouth'', you can just as well shut up and fuck off. -[[User:DePiep|DePiep]] ([[User talk:DePiep|talk]]) 11:08, 2 December 2014 (UTC)\n",
      "\n",
      "page_contents/Talk:Russia.txt GreenMeansGo 0.9062575750097962  :::Yeah, well I can call Joe Biden a big dumb idiot, and say he's too old to be using Legos, and that his feet probably smell bad, and somehow, I have no fear I'll be poisoned or haphazardly fall out a window. [[User:GreenMeansGo|<span style=\"font-family:Impact\"><span style=\"color:#07CB4B\">G</span><span style=\"color:#449351\">M</span><span style=\"color:#35683d\">G</span></span>]][[User talk:GreenMeansGo#top|<sup style=\"color:#000;font-family:Impact\">talk</sup>]] 12:50, 11 August 2023 (UTC)\n",
      "\n",
      "page_contents/Talk:Association football.txt 78.86.130.228 0.9946258023660924  \n",
      " :::::::::::::It's called football. You people are ridiculous. Fifa. No s. Stupid Americans [[Special:Contributions/78.86.130.228|78.86.130.228]] ([[User talk:78.86.130.228|talk]]) 20:26, 17 May 2023 (UTC)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output all offensive comments\n",
    "authors = []\n",
    "for row in df_comments[df_comments[\"toxic\"] > 0.9].iterrows():\n",
    "    if row[1][\"Author\"]:\n",
    "        authors.append(row[1][\"Author\"])\n",
    "    print(row[1][\"Filename\"], row[1][\"Author\"], row[1][\"toxic\"], row[1][\"Comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "multiple_offenders = [x for x in authors if x in seen or seen.add(x)]    \n",
    "\n",
    "# number of people that have written multiple toxic comments\n",
    "len(set(multiple_offenders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all toxic comments of multiple offenders\n",
    "for row in df_comments[df_comments[\"toxic\"] > 0.9].iterrows():\n",
    "    if row[1][\"Author\"] in multiple_offenders:\n",
    "        print(f'{row[1][\"Filename\"]}, Author: {row[1][\"Author\"]}, Comment: {\" \".join(word_tokenize(row[1][\"Comment\"]))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
