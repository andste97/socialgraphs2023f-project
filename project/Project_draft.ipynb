{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data analysis metrics\n",
    "\n",
    "> An explanation of the central idea behind your final project (What is the idea? Why is it interesting? Which datasets did you need to explore the idea? How did you download them?)\n",
    "\n",
    "Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up outputs from warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scraper\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist\n",
    "import pathlib\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If something related to tqdm fails, run:\n",
    "> `pip install ipywidgets widgetsnbextension pandas-profiling`\n",
    "\n",
    "Changes?\n",
    "> `pip freeze > requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_titles = [\n",
    "    \"Category:Wikipedia_level-1_vital_articles\",\n",
    "    \"Category:Wikipedia_level-2_vital_articles\",\n",
    "    \"Category:Wikipedia_level-3_vital_articles\",\n",
    "    #\"Category:Wikipedia_level-4_vital_articles\",\n",
    "    #\"Category:Wikipedia_level-5_vital_articles\"\n",
    "]\n",
    "\n",
    "page_graph, infos  = await scraper.scrape_wiki(category_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A walk-through of your preliminary data analysis, addressing:\n",
    "> - What is the total size of your data? (MB, number of pages, other variables, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_pages = [title for title in page_graph.nodes if title[:5] == \"Talk:\"]\n",
    "users = [title for title in page_graph.nodes if title[:5] == \"User:\"]\n",
    "print(\"Number of pages in the vital articles dataset:\", len(infos[\"titles\"]))\n",
    "print(\"Number of related archived pages:\", len(infos[\"archive_titles\"]))\n",
    "print(\"Number of users found in relation to the dataset:\", len(users))\n",
    "print(\"No. nodes\", page_graph.number_of_nodes())\n",
    "print(\"No. links:\", page_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - What is the network you will be analyzing? (number of nodes? number of links?, degree distributions, what are node attributes?, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = 'DejaVu Sans'\n",
    "\n",
    "graph = page_graph.copy()\n",
    "\n",
    "# Only keep users with a high degree\n",
    "for node in page_graph.nodes(data=True):\n",
    "    if \"page_class\" in node[1]:\n",
    "        if node[1][\"page_class\"] == \"user\":\n",
    "            if page_graph.out_degree(node[0]) <= 10:\n",
    "                graph.remove_node(node[0])\n",
    "    else:\n",
    "        graph.remove_node(node[0])\n",
    "\n",
    "# Remove outliers\n",
    "cc = nx.weakly_connected_components(graph)\n",
    "largest_c = max(cc, key=lambda x: len(x))\n",
    "rsubgraph = nx.subgraph(graph, largest_c)\n",
    "\n",
    "# Positions (currently unused)\n",
    "#pos = nx.nx_agraph.graphviz_layout(rsubgraph, prog=\"neato\")\n",
    "\n",
    "# Color and size according to coast and degree\n",
    "color_talk = \"#0000FF\"\n",
    "color_user = \"#FF0000\"\n",
    "node_colors = [color_talk if node[1][\"page_class\"] == \"talk\" else color_user for node in rsubgraph.nodes(data=True)]\n",
    "node_sizes = [rsubgraph.degree(node) for node in rsubgraph.nodes]\n",
    "\n",
    "nx.draw(rsubgraph, with_labels=True, font_weight='light', font_size=5, node_size=node_sizes, width=.1, edge_color=\"#555555\", arrowsize=2, node_color=node_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(rsubgraph.degree, key=lambda item: item[1], reverse=True)[:10]\n",
    "import pandas as pd\n",
    "\n",
    "degrees = pd.DataFrame(rsubgraph.degree, columns=[\"Node\", \"Degree\"])\n",
    "degrees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees[\"PageType\"] = [\"User\" if node[:5] == \"User:\" else \"Talk\" for node in degrees.Node ]\n",
    "users = degrees[degrees.PageType == \"User\"]\n",
    "potential_bots = users[users[\"Node\"].str.contains('bot', case=False)]\n",
    "print(f\"{len(potential_bots)} users found with with bot in their name:\")\n",
    "print(\",\\n\".join(potential_bots.Node.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluation we found that `User:Botteville`, `User:KP Botany` and `User:NinjaRobotPirate` are human users. Therefore we can filter out the bots by name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_names = [\"User:Community Tech bot\", \"User:PrimeBOT\", \"User:InternetArchiveBot\", \"User:AnomieBOT\", \"User:RMCD bot\", \"User:Cyberbot II\", \"User:CommonsNotificationBot\",\n",
    "\"User:LinkBot\", \"User:FairuseBot\", \"User:BetacommandBot\", \"User:Legobot\", \"User:DumZiBoT\"]\n",
    "\n",
    "human_users = users[~users[\"Node\"].isin(bot_names)]\n",
    "n = 10\n",
    "top_n_human_df = human_users.sort_values([\"Degree\"], ascending=False).head(n)\n",
    "top_n_human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make subgraph with users and their related pages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_human_names = top_n_human_df.Node.values\n",
    "top_users_graph = graph.subgraph(sum([list(graph.neighbors(node)) + [node] for node in top_n_human_names], []))\n",
    "pages = [node for node in top_users_graph.nodes if node not in top_n_human_names] \n",
    "\n",
    "node_colors = [color_talk if node[1][\"page_class\"] == \"talk\" else color_user for node in top_users_graph.nodes(data=True)]\n",
    "node_sizes = [rsubgraph.degree(node) for node in top_users_graph.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(top_users_graph.degree())\n",
    "sorted_nodes = sorted(degrees, key=degrees.get, reverse=True)\n",
    "top_nodes_count = 10\n",
    "top_nodes = [node for node in sorted(degrees, key=degrees.get, reverse=True) if node[:5] == \"Talk:\"][:top_nodes_count]\n",
    "labels = {node: node for node in top_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fa2 import ForceAtlas2\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=2.0,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=3.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=0.1,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "pos=forceatlas2.forceatlas2_networkx_layout(top_users_graph, pos=None, iterations=2000)\n",
    "    \n",
    "nx.draw(top_users_graph, pos=pos, node_color=node_colors, node_size=node_sizes, edge_color=\"#999999\", width=0.3, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(top_users_graph)\n",
    "label_pos = {node: (pos[node][0], pos[node][1] + 0.15) for node in top_nodes}\n",
    "nx.draw(top_users_graph, pos=pos, node_color=node_colors, node_size=node_sizes, edge_color=\"#999999\", width=0.3, alpha=0.5, with_labels=False)\n",
    "nx.draw_networkx_labels(top_users_graph, pos, labels=labels, font_size=10, font_color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "import scipy.stats as sps\n",
    "\n",
    "# Basic Statistics\n",
    "\n",
    "count_nodes = len(page_graph)\n",
    "count_edges = len(page_graph.edges())\n",
    "\n",
    "# Create degree statistic dicts\n",
    "degrees = dict(page_graph.degree())\n",
    "in_degrees = dict(page_graph.in_degree())\n",
    "out_degrees = dict(page_graph.out_degree())\n",
    "\n",
    "talk_page_in_degrees = {k: v for k, v in in_degrees.items() if page_graph.nodes[k][\"page_class\"] == \"talk\"}\n",
    "user_out_degrees = {k: v for k, v in out_degrees.items() if page_graph.nodes[k][\"page_class\"] == \"user\"}\n",
    "\n",
    "def find_top(n, stat_dict):\n",
    "    degrees_pages = []\n",
    "    degrees_users = []\n",
    "    top_overall = \"\"\n",
    "\n",
    "    for page, degree in dict(sorted(stat_dict.items(), key=lambda item: item[1], reverse=True)).items():\n",
    "        if top_overall == \"\":\n",
    "            top_overall = page + \" - \" + str(degree)\n",
    "\n",
    "        # stat dicts don't distinguish between east/west, so we'll do that here\n",
    "        if len(degrees_pages) < n and page_graph.nodes[page][\"page_class\"] == \"talk\": \n",
    "            degrees_pages.append(page + \" - \" + str(degree))\n",
    "        elif len(degrees_users) < n and page_graph.nodes[page][\"page_class\"] == \"user\":\n",
    "            degrees_users.append(page + \" - \" + str(degree))\n",
    "\n",
    "        if len(degrees_pages) >= n and len(degrees_users) >= n:\n",
    "            break  # found all top v\n",
    "    \n",
    "    return degrees_pages, degrees_users, top_overall\n",
    "\n",
    "degrees_pages, degrees_users, top_overall = find_top(10, degrees)\n",
    "\n",
    "print(\"Number of nodes: \" + str(count_nodes))\n",
    "print(\"Number of links: \" + str(count_edges))\n",
    "\n",
    "print()\n",
    "print(\"Highest degrees for pages:\")\n",
    "print(\"> Overall:\")\n",
    "print(top_overall)\n",
    "print(\"> Pages:\")\n",
    "print(\"\\n\".join(degrees_pages))\n",
    "print(\"> Users:\")\n",
    "print(\"\\n\".join(degrees_users))\n",
    "\n",
    "\n",
    "# Degree multiplicities\n",
    "in_degrees_counts = p.Series(talk_page_in_degrees.values()).value_counts()\n",
    "out_degrees_counts = p.Series(user_out_degrees.values()).value_counts()\n",
    "\n",
    "max_degree = max([max(in_degrees_counts.index), max(out_degrees_counts.index)])\n",
    "max_multiplicity = max([max(in_degrees_counts.values), max(out_degrees_counts.values)])\n",
    "range_x = range(1, max_degree + 1)\n",
    "\n",
    "in_degrees_counts_interp = in_degrees_counts.reindex(range(max_degree+1), fill_value=0).sort_index()\n",
    "out_degrees_counts_interp = out_degrees_counts.reindex(range(max_degree+1), fill_value=0).sort_index()\n",
    "\n",
    "# Exponents\n",
    "fit_in = powerlaw.Fit(in_degrees_counts.sort_index().values, verbose=False)\n",
    "fit_out = powerlaw.Fit(out_degrees_counts.sort_index().values, verbose=False)\n",
    "\n",
    "exp_in = fit_in.alpha\n",
    "exp_out = fit_out.alpha\n",
    "\n",
    "print(\"Exponents:\")\n",
    "print(\"In-degrees: \" + str(exp_in) + \" sigma: \" + str(fit_in.sigma))\n",
    "print(\"Out-degrees: \" + str(exp_out) + \" sigma: \" + str(fit_out.sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def fpl(x, a):\n",
    "    return x ** (-a)\n",
    "\n",
    "# Plots\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "# Sturges rule\n",
    "no_bins_sturges = int(1 + math.log(len(talk_page_in_degrees.values()), 2))\n",
    "\n",
    "axs[0, 0].scatter(in_degrees_counts.index, in_degrees_counts.values, s=5, label='Data')\n",
    "#hist, bin_edges, _ = axs[0, 0].hist(talk_page_in_degrees.values(), bins=no_bins_sturges, edgecolor='white', label='Data')\n",
    "#axs[0, 0].set_xticks(bin_edges)\n",
    "#axs[0, 0].set_xticklabels(['%.0f' % val for val in bin_edges], rotation=45)\n",
    "axs[0,0].set_title('Multiplicity of In-degrees for Talk pages')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].scatter(in_degrees_counts.index, in_degrees_counts.values, s=5, label='Data')\n",
    "#axs[0,1].plot(range_x, fpl(range_x, exp_in) * count_nodes, 'k-', lw=1, alpha=.75, label='Power Law fit')\n",
    "axs[0,1].set_yscale('log')\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,1].set_title('Multiplicity of In-degrees for Talk pages [log-log]')\n",
    "axs[0,1].set_xlim(1, max_degree)\n",
    "axs[0,1].set_ylim(1, max_multiplicity)\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1, 0].scatter(out_degrees_counts.index, out_degrees_counts.values, color=\"red\", s=5, label='Data')\n",
    "# hist, bin_edges, _ = axs[1, 0].hist(talk_page_in_degrees.values(), bins=no_bins_sturges, color=\"red\", edgecolor='white', label='Data')\n",
    "# axs[1, 0].set_xticks(bin_edges)\n",
    "# axs[1, 0].set_xticklabels(['%.0f' % val for val in bin_edges], rotation=45)\n",
    "axs[1,0].set_title('Multiplicity of Out-degrees for Users')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].scatter(out_degrees_counts.index, out_degrees_counts.values, s=5, label='Data', color=\"red\")\n",
    "#axs[1,1].plot(range_x, fpl(range_x, exp_out) * count_nodes, 'k-', lw=1, alpha=.75, label='Power Law fit')\n",
    "axs[1,1].set_yscale('log')\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,1].set_title('Multiplicity of Out-degrees for Users [log-log]')\n",
    "axs[1,1].set_xlim(1, max_degree)\n",
    "axs[1,1].set_ylim(1, max_multiplicity)\n",
    "axs[1,1].legend()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Degree', ylabel='Multiplicity')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis per user\n",
    "\n",
    "For this, we will first need to extract all the comments from all pages, as well as the author of the comment\n",
    "Then we will assign all comment texts to a single author, and run sentiment analysis on the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipage_folder = pathlib.Path(\"./page_contents/\")\n",
    "filenames = list(wikipage_folder.rglob(\"*.txt\"))\n",
    "\n",
    "#worker_results = [parse_comments_from_pages(filenames[:100])]\n",
    "\n",
    "with Pool(12) as pool:\n",
    "    # perform calculations\n",
    "    worker_results = pool.map(utils.parse_comments_from_pages, utils.chunk_list(filenames, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict = {}\n",
    "list_for_df = []\n",
    "\n",
    "# iterate over the results by the workers\n",
    "# and transform the output into a dictionary with the users as keys\n",
    "# and their comments as text\n",
    "for worker_result in worker_results:\n",
    "    for filename, page in worker_result:\n",
    "        for subsection in page[\"sections\"]:\n",
    "            if subsection.get(\"heading\"):\n",
    "                for comments in subsection.get(\"comments\"):\n",
    "                    if comments.get(\"author\"):\n",
    "                        for author, comment in utils.parse_comment_subcomment(comments):\n",
    "                            if author not in author_dict:\n",
    "                                author_dict[author] = []    \n",
    "                            author_dict[author].append(comment) # this will concatenate the arrays. \n",
    "                            list_for_df.append([author, comment, filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "# show the top 5 authors written the most text in comment pages\n",
    "# before tokenizing the comments\n",
    "items = author_dict.items()\n",
    "items_sorted = sorted(items, key=lambda x: len(x[1]), reverse=True)\n",
    "[(author, len(utils.flatten(comments))) for author, comments in items_sorted][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT = pd.read_csv(\"./labMT.txt\", sep=\"\\t\")\n",
    "# to facilitate happiness_average value lookup\n",
    "labMT.set_index(\"word\", inplace=True)\n",
    "\n",
    "# Do sentiment analysis\n",
    "# code taken from assignment 2\n",
    "\n",
    "def sentiment(tokens):\n",
    "    if(len(tokens) == 0):\n",
    "        return\n",
    "    freq = FreqDist(tokens)\n",
    "\n",
    "    # filter for the vocabulary we can evaluate with LabMT\n",
    "    vocab = list(filter(lambda word: word in labMT.index, np.unique(tokens)))\n",
    "\n",
    "    # array of each token's average happiness weighted by the token's frequency\n",
    "    weighted_happiness = np.fromiter((freq[word] * labMT.loc[word].happiness_average for word in vocab), dtype=float)\n",
    "    # each token's frequency\n",
    "    word_frequencies = np.fromiter((freq[word] for word in vocab), dtype=float)\n",
    "    return np.sum(weighted_happiness) / np.sum(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = {}\n",
    "for author, text in author_dict.items():\n",
    "    text = [utils.tokenize_custom(s) for s in text]\n",
    "    text = utils.flatten(text)\n",
    "    # compute sentiment for individual rapper wiki page\n",
    "    if len(text) > 100:\n",
    "        sentiment_value = sentiment(text)\n",
    "        if(sentiment_value):\n",
    "            sentiments[author] = sentiment_value\n",
    "\n",
    "sentiments_df = pd.DataFrame({\"Author\": sentiments.keys(), \"comment_happiness\": sentiments.values()})\n",
    "# use the author's name as index\n",
    "sentiments_df.set_index(\"Author\", inplace=True)\n",
    "sentiments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 authors with the happiest comments:\")\n",
    "best_sentiments = sentiments_df.sort_values(\"comment_happiness\", ascending=False).head(10)\n",
    "best_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 authors with the saddest comments:\")\n",
    "worst_sentiments = sentiments_df.sort_values(\"comment_happiness\", ascending=True).head(10)\n",
    "worst_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the original comment before tokenizin\n",
    "[s for s in author_dict.get(best_sentiments.iloc[0].name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in author_dict.get(worst_sentiments.iloc[0].name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from labmt statistical sentiment analysis\n",
    "\n",
    "These results show, that it is not beneficial to use the labMT approach to find toxic comments. Different methods should be used for our problem.\n",
    "\n",
    "What we have seen is that comment sentiment mostly correlates with the topic they are discussing. E.g.: topics discussing the pages fear, Nazi, Hitler have a very low score, but comments discussing e.g. happiness, or the great barrier reef have very high sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.DataFrame(list_for_df, columns = ['Author', 'Comment', 'Filename']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for loading pickles\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dictionary values\n",
    "#flattened_data = [(key, value) for key, values in author_dict.items() for value in values]\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "#df = pd.DataFrame(flattened_data, columns=['Author', 'Comment'])\n",
    "\n",
    "# load the vectorizer\n",
    "with open('./sentiment-models/vectorizer.pkl', 'rb') as file:\n",
    "    vec = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse TF-IDF matrix with vectorizer trained on kaggle toxic comment dataset\n",
    "\n",
    "comments_sparse = vec.transform(df_comments['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify comments according to the following categories:\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "preds = np.zeros((len(df_comments), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('predict', j)\n",
    "    with open(f'./sentiment-models/{j}.pkl', 'rb') as file:\n",
    "        m,r = pickle.load(file)\n",
    "    preds[:,i] = m.predict_proba(comments_sparse.multiply(r))[:,1]\n",
    "    df_comments[j] = preds[:,i]\n",
    "\n",
    "# this created a matrix where each new column for label_cols contains the probability that a comment is in\n",
    "# the category with the same title as the column name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all offensive comments\n",
    "authors = []\n",
    "for row in df_comments[df_comments[\"toxic\"] > 0.9].iterrows():\n",
    "    if row[1][\"Author\"]:\n",
    "        authors.append(row[1][\"Author\"])\n",
    "    print(row[1][\"Filename\"], row[1][\"Author\"], row[1][\"toxic\"], row[1][\"Comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "multiple_offenders = [x for x in authors if x in seen or seen.add(x)]    \n",
    "\n",
    "# number of people that have written multiple toxic comments\n",
    "len(set(multiple_offenders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all toxic comments of multiple offenders\n",
    "for row in df_comments[df_comments[\"toxic\"] > 0.9].iterrows():\n",
    "    if row[1][\"Author\"] in multiple_offenders:\n",
    "        print(f'{row[1][\"Filename\"]}, Author: {row[1][\"Author\"]}, Comment: {\" \".join(word_tokenize(row[1][\"Comment\"]))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
