{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_talk</th>\n",
       "      <th>raw_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald_Trump</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n\\n\\n&lt;/p&gt;\\n&lt;link rel=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald_Trump</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;link rel=\"mw-deduplicated-inline-style\" href=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barack_Obama</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;link rel=\"mw-deduplicated-inline-style\" href=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barack_Obama</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;link rel=\"mw-deduplicated-inline-style\" href=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe_Biden</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n\\n\\n&lt;/p&gt;\\n&lt;p&gt;&lt;b&gt;Jose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  is_talk                                        raw_content\n",
       "0  Donald_Trump    False  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...\n",
       "1  Donald_Trump     True  <link rel=\"mw-deduplicated-inline-style\" href=...\n",
       "2  Barack_Obama    False  <link rel=\"mw-deduplicated-inline-style\" href=...\n",
       "3  Barack_Obama     True  <link rel=\"mw-deduplicated-inline-style\" href=...\n",
       "4     Joe_Biden    False  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<p><b>Jose..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import urllib.request as request\n",
    "import urllib\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def download_article_revisions(article_name: str):\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\" + urllib.parse.quote(article_name)\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat =\"format=json\"\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "\n",
    "    wikiresponse = request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "        # page content\n",
    "    for _, value in json.loads(wikitext)[\"query\"][\"pages\"].items():\n",
    "         return value[\"revisions\"][0][\"*\"]\n",
    "\n",
    "\n",
    "def download_article_extracts(article_name: str):\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\" + urllib.parse.quote_plus(article_name)\n",
    "    content = \"prop=extracts\"\n",
    "    exlimit = \"exlimit=1\"\n",
    "    explaintext = \"explaintext=1\"\n",
    "    dataformat =\"format=json\"\n",
    "\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "\n",
    "    wikiresponse = request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "        # page content\n",
    "    for _, value in json.loads(wikitext)[\"query\"][\"pages\"].items():\n",
    "            return value [\"extract\"] \n",
    "\n",
    "ids = [\"Donald_Trump\", \"Barack_Obama\", \"Joe_Biden\"]\n",
    "wiki_ids = []\n",
    "raw_texts = []\n",
    "isTalk = []\n",
    "for id in ids:\n",
    "     talk_id = f\"Talk:{id}\"\n",
    "\n",
    "     wiki_ids.append(id)\n",
    "     isTalk.append(False)\n",
    "     raw_texts.append(download_article_extracts(id))\n",
    "\n",
    "     wiki_ids.append(id)\n",
    "     isTalk.append(True)\n",
    "     raw_texts.append(download_article_extracts(talk_id))\n",
    "\n",
    "wiki_pages = pd.DataFrame({\"id\": wiki_ids, \"is_talk\": isTalk, \"raw_content\": raw_texts})\n",
    "wiki_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize, Text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens = []\n",
    "wnl = WordNetLemmatizer()\n",
    "for raw in wiki_pages.raw_content:\n",
    "    # tokenization and lemmatization of words (filtering out punctuation)\n",
    "    processed = list(filter(lambda token: wnl.lemmatize(token.lower()) not in string.punctuation, word_tokenize(raw)))\n",
    "    tokens.append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages[\"tokens\"] = tokens\n",
    "labMT = pd.read_csv(\"../assignments/labMT.txt\", sep=\"\\t\")\n",
    "# to facilitate happiness_average value lookup\n",
    "labMT.set_index(\"word\", inplace=True)\n",
    "# labMT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(tokens):\n",
    "    if(len(tokens) == 0):\n",
    "        return\n",
    "    freq = FreqDist(tokens)\n",
    "\n",
    "    # filter for the vocabulary we can evaluate with LabMT\n",
    "    vocab = list(filter(lambda word: word in labMT.index, np.unique(tokens)))\n",
    "\n",
    "    # array of each token's average happiness weighted by the token's frequency\n",
    "    weighted_happiness = np.fromiter((freq[word] * labMT.loc[word].happiness_average for word in vocab), dtype=float)\n",
    "    # each token's frequency\n",
    "    word_frequencies = np.fromiter((freq[word] for word in vocab), dtype=float)\n",
    "    return np.sum(weighted_happiness) / np.sum(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_talk</th>\n",
       "      <th>raw_content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Donald_Trump</th>\n",
       "      <td>False</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n\\n\\n&lt;/p&gt;\\n&lt;link rel=...</td>\n",
       "      <td>[p, class=, '', mw-empty-elt, '', /p, link, re...</td>\n",
       "      <td>5.224624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donald_Trump</th>\n",
       "      <td>True</td>\n",
       "      <td>&lt;link rel=\"mw-deduplicated-inline-style\" href=...</td>\n",
       "      <td>[link, rel=, '', mw-deduplicated-inline-style,...</td>\n",
       "      <td>5.413483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barack_Obama</th>\n",
       "      <td>False</td>\n",
       "      <td>&lt;link rel=\"mw-deduplicated-inline-style\" href=...</td>\n",
       "      <td>[link, rel=, '', mw-deduplicated-inline-style,...</td>\n",
       "      <td>5.321338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barack_Obama</th>\n",
       "      <td>True</td>\n",
       "      <td>&lt;link rel=\"mw-deduplicated-inline-style\" href=...</td>\n",
       "      <td>[link, rel=, '', mw-deduplicated-inline-style,...</td>\n",
       "      <td>5.377172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joe_Biden</th>\n",
       "      <td>False</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n\\n\\n&lt;/p&gt;\\n&lt;p&gt;&lt;b&gt;Jose...</td>\n",
       "      <td>[p, class=, '', mw-empty-elt, '', /p, p, b, Jo...</td>\n",
       "      <td>5.269958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joe_Biden</th>\n",
       "      <td>True</td>\n",
       "      <td>&lt;p&gt;&lt;br&gt;&lt;/p&gt;\\n&lt;link rel=\"mw-deduplicated-inline...</td>\n",
       "      <td>[p, br, /p, link, rel=, '', mw-deduplicated-in...</td>\n",
       "      <td>5.396449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              is_talk                                        raw_content  \\\n",
       "id                                                                         \n",
       "Donald_Trump    False  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...   \n",
       "Donald_Trump     True  <link rel=\"mw-deduplicated-inline-style\" href=...   \n",
       "Barack_Obama    False  <link rel=\"mw-deduplicated-inline-style\" href=...   \n",
       "Barack_Obama     True  <link rel=\"mw-deduplicated-inline-style\" href=...   \n",
       "Joe_Biden       False  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<p><b>Jose...   \n",
       "Joe_Biden        True  <p><br></p>\\n<link rel=\"mw-deduplicated-inline...   \n",
       "\n",
       "                                                         tokens  sentiment  \n",
       "id                                                                          \n",
       "Donald_Trump  [p, class=, '', mw-empty-elt, '', /p, link, re...   5.224624  \n",
       "Donald_Trump  [link, rel=, '', mw-deduplicated-inline-style,...   5.413483  \n",
       "Barack_Obama  [link, rel=, '', mw-deduplicated-inline-style,...   5.321338  \n",
       "Barack_Obama  [link, rel=, '', mw-deduplicated-inline-style,...   5.377172  \n",
       "Joe_Biden     [p, class=, '', mw-empty-elt, '', /p, p, b, Jo...   5.269958  \n",
       "Joe_Biden     [p, br, /p, link, rel=, '', mw-deduplicated-in...   5.396449  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "sentiments = []\n",
    "for token_list in wiki_pages.tokens:\n",
    "    # compute sentiment for individual rapper wiki page\n",
    "    sentiment_value = sentiment(token_list)\n",
    "    if(sentiment_value):\n",
    "        sentiments.append(sentiment_value)\n",
    "\n",
    "wiki_pages[\"sentiment\"] = sentiments\n",
    "# use the rappers' name as index\n",
    "# wiki_pages.set_index(\"id\", inplace=True)\n",
    "wiki_pages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token number in Donald_Trump talk page: 17743\n",
      "Sentiment in Donald_Trump talk page: 5.4134830371567055\n",
      "\n",
      "Token number in Barack_Obama talk page: 5375\n",
      "Sentiment in Barack_Obama talk page: 5.377172330097087\n",
      "\n",
      "Token number in Joe_Biden talk page: 9231\n",
      "Sentiment in Joe_Biden talk page: 5.396449351559761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "talks = wiki_pages[wiki_pages.is_talk == True]\n",
    "articles = wiki_pages[wiki_pages.is_talk == False]\n",
    "\n",
    "for id in talks.index:\n",
    "    print(f\"Token number in {id} talk page: {len(talks.loc[id].tokens)}\")\n",
    "    print(f\"Sentiment in {id} talk page:\", talks.loc[id].sentiment)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token number in Donald_Trump article page: 20994\n",
      "Sentiment in Donald_Trump article page: 5.224624036495202\n",
      "\n",
      "Token number in Barack_Obama article page: 14778\n",
      "Sentiment in Barack_Obama article page: 5.321338260375213\n",
      "\n",
      "Token number in Joe_Biden article page: 13929\n",
      "Sentiment in Joe_Biden article page: 5.2699580468476865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id in articles.index:\n",
    "    print(f\"Token number in {id} article page: {len(articles.loc[id].tokens)}\")\n",
    "    print(f\"Sentiment in {id} article page:\", articles.loc[id].sentiment)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
